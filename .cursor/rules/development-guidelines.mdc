---
description: 
globs: 
alwaysApply: true
---
# Development Guidelines

## Core Principles - NEVER VIOLATE
1. **LLM-First Design**: All game reasoning must come from language models, not hardcoded logic
2. **No Hardcoded Solutions**: Never hardcode game mechanics, location checks, or puzzle solutions  
3. **Parser Validation Only**: The only acceptable hardcoded check is validating if Zork accepted a command
4. **No Fallback Mechanisms**: When LLMs fail, improve prompts/models rather than adding fallbacks
5. **Genuine AI Play**: The system should demonstrate authentic LLM capabilities

## Development Best Practices

### When Adding New Features
- **Maintain LLM Autonomy**: Ensure new features support rather than replace LLM reasoning
- **Structured Data**: Use Pydantic models for type safety and validation
- **Comprehensive Logging**: Log all new interactions and state changes
- **Memory Integration**: Ensure new data flows through the memory system appropriately
- **Configuration Integration**: Add new settings to `pyproject.toml` using the established structure

### When Debugging Issues
- **Check Logs First**: Use [log_tools.py](mdc:log_tools.py) to analyze episode behavior
- **Prompt Engineering**: Improve system prompts in [agent.md](mdc:agent.md), [critic.md](mdc:critic.md), [extractor.md](mdc:extractor.md)
- **Model Selection**: Try different LLM models in [config.py](mdc:config.py) before adding logic
- **Parameter Tuning**: Adjust sampling parameters in `pyproject.toml` per component
- **Knowledge Quality**: Check adaptive knowledge updates for degradation patterns

### When Extending the System
- **Adaptive Knowledge**: Use [zork_strategy_generator.py](mdc:zork_strategy_generator.py) for continuous learning
- **Memory Enhancement**: Extend memory retrieval in `get_relevant_memories_for_prompt()`
- **Map Intelligence**: Enhance spatial reasoning through [map_graph.py](mdc:map_graph.py)
- **State Tracking**: Ensure new data is captured in action_reasoning_history for analysis
- **Extended Sessions**: Design features for multi-thousand turn gameplay sessions

## Key Files & Responsibilities

### Core Orchestration
- [zork_orchestrator.py](mdc:zork_orchestrator.py): Central coordinator - manage session flow, periodic updates
- [main.py](mdc:main.py): Entry point - keep minimal, use configuration system
- [config.py](mdc:config.py): Configuration loader - add new Pydantic models here

### LLM Components (Never add hardcoded logic here)
- [zork_agent.py](mdc:zork_agent.py): Action generation - improve prompts, not fallbacks
- [hybrid_zork_extractor.py](mdc:hybrid_zork_extractor.py): Information extraction - trust LLM parsing
- [zork_critic.py](mdc:zork_critic.py): Action evaluation - tune trust tracking, not override logic
- [zork_strategy_generator.py](mdc:zork_strategy_generator.py): Knowledge management - let LLM assess quality

### Supporting Infrastructure
- [zork_api.py](mdc:zork_api.py): Game interface - minimal changes, robust error handling
- [map_graph.py](mdc:map_graph.py): Spatial tracking - confidence-based, no hardcoded locations
- [movement_analyzer.py](mdc:movement_analyzer.py): Pattern analysis - statistical, not rule-based
- [logger.py](mdc:logger.py): Event tracking - extend for new data, maintain JSON structure

## Configuration Management

### Adding New Configuration
1. **Create Pydantic Model** in [config.py](mdc:config.py):
   ```python
   class NewFeatureConfig(BaseModel):
       setting_name: str = "default_value"
       numeric_setting: float = 1.0
   ```

2. **Add to Main Config**:
   ```python
   class ZorkGPTConfig(BaseModel):
       new_feature: NewFeatureConfig = NewFeatureConfig()
   ```

3. **Update pyproject.toml**:
   ```toml
   [tool.zorkgpt.new_feature]
   setting_name = "custom_value"
   numeric_setting = 2.5
   ```

4. **Use in Code**:
   ```python
   config = get_config()
   value = config.new_feature.setting_name
   ```

### Configuration Precedence
Constructor parameters > pyproject.toml > Pydantic defaults

### Environment Variables
**ONLY** use environment variables for:
- `CLIENT_API_KEY`: LLM API authentication
- `AWS_PROFILE`: Set to `parrishfamily` for deployments

Everything else goes in `pyproject.toml`.

## LLM Component Guidelines

### Prompt Engineering (Preferred Approach)
- **System Prompts**: Update .md files for behavior changes
- **Context Assembly**: Improve memory/knowledge integration
- **Sampling Parameters**: Tune temperature/top_p in config per component
- **Model Selection**: Try different models before adding logic

### Trust & Evaluation
- **Critic Scores**: Use for action filtering, not hardcoded rules
- **Confidence Tracking**: Build on existing trust systems
- **Quality Assessment**: Let LLMs evaluate their own performance
- **Rejection Thresholds**: Tune via configuration, not code

### Memory & Knowledge Integration
- **Memory Retrieval**: Extend `get_relevant_memories_for_prompt()`
- **Knowledge Updates**: Work within the 100-turn window system
- **Map Integration**: Use confidence scores, not location databases
- **Context Windows**: Optimize for extended sessions

## Testing Strategy

### Test Organization
- **Scenario Tests**: Add to [tests/](mdc:tests) for new gameplay features
- **Component Tests**: Unit tests for LLM component interactions
- **Configuration Tests**: Validate pyproject.toml loading
- **Session Tests**: Test extended session features (use smaller turn limits)

### Testing Extended Sessions
- **Mock Long Sessions**: Use shorter turn limits for faster testing
- **Knowledge Persistence**: Verify learning across session boundaries
- **Memory Efficiency**: Test with large memory_log_history
- **State Export**: Validate JSON structure and S3 integration

### Log Analysis Testing
- **Turn Window Analysis**: Verify 100-turn knowledge extraction
- **Performance Tracking**: Test action_reasoning_history
- **Adaptive Updates**: Mock LLM responses for knowledge quality assessment

## Performance Optimization

### Extended Session Considerations
- **Memory Management**: Optimize retrieval for 5000+ turn sessions
- **Knowledge Updates**: Balance frequency (100 turns) with computational cost
- **Map Updates**: More frequent spatial updates (25 turns) for navigation
- **State Export**: Efficient JSON serialization for real-time monitoring

### Component-Specific Tuning
- **Agent Sampling**: Higher temperature (0.5) for creativity
- **Critic Sampling**: Lower temperature (0.2) for consistency
- **Extractor Sampling**: Minimal temperature (0.1) for accuracy
- **Strategy Analysis**: Use powerful models (GPT-4) for synthesis

## Common Anti-Patterns to Avoid

### Hardcoded Logic (Never Do This)
```python
❌ if "Forest" in game_text: return "north"
❌ if action_failed: return "look"
❌ if puzzle_detected: return hardcoded_solution
❌ if score < 100: prioritize_treasure_hunting()
```

### LLM-First Alternatives (Do This Instead)
```python
✅ extracted_info = extractor.extract_info(game_text)
✅ action = agent.get_action_with_reasoning(context)
✅ critic_score = critic.evaluate_action(action, context)
✅ knowledge_manager.update_from_experience(turn_data)
```

### Configuration Anti-Patterns
```python
❌ os.environ["NEW_SETTING"] = "value"  # Use pyproject.toml
❌ if not config_exists: use_fallback()  # Use Pydantic defaults
❌ hardcoded_timeout = 30  # Use configuration
```

### Knowledge Management Anti-Patterns
```python
❌ reset_knowledge_between_episodes()  # Maintain continuity
❌ if turn_count % 1000 == 0: force_update()  # Let LLM assess quality
❌ manual_knowledge_curation()  # Trust LLM knowledge merging
```

## Code Quality Standards

### Type Safety
- Use Pydantic models for all configuration
- Add type hints to new functions
- Validate data structures at boundaries

### Error Handling
- Graceful degradation without breaking LLM-first principle
- Comprehensive logging for debugging
- Timeout handling for LLM calls

### Documentation
- Update .md prompt files for behavior changes
- Document new configuration options
- Add docstrings for complex functions

## Deployment & Infrastructure

### AWS Requirements
- **Profile**: Always use `parrishfamily` profile
- **Environment**: `AWS_PROFILE=parrishfamily` for [deploy.py](mdc:infrastructure/deploy.py)
- **CLI Usage**: `--profile=parrishfamily` for direct awscli commands

### Development Environment
- **Dependencies**: Use `uv` for package management
- **Node Version**: `nvm use lts/iron` before CDK operations
- **Configuration**: Validate with `python -c "from config import get_config; print(get_config())"`

### Pre-deployment Checklist
1. All settings in `pyproject.toml` (not environment variables)
2. Configuration validation passes
3. Extended session tests complete
4. S3 export configuration verified (if using live monitoring)

### EC2 Instance Management
Use [manage_ec2.py](mdc:infrastructure/manage_ec2.py) for remote EC2 operations:

**Service Management**:
- `python infrastructure/manage_ec2.py status` - Check ZorkGPT service status
- `python infrastructure/manage_ec2.py start` - Start the ZorkGPT service
- `python infrastructure/manage_ec2.py stop` - Stop the ZorkGPT service  
- `python infrastructure/manage_ec2.py restart` - Restart the ZorkGPT service

**Monitoring & Debugging**:
- `python infrastructure/manage_ec2.py logs` - View recent service logs
- `python infrastructure/manage_ec2.py logs-follow` - Follow logs in real-time
- `python infrastructure/manage_ec2.py download` - Download analysis files for local review

**Maintenance**:
- `python infrastructure/manage_ec2.py update` - Update ZorkGPT to latest version
- `python infrastructure/manage_ec2.py ssh` - Open interactive SSH session
- `python infrastructure/manage_ec2.py info` - Display instance connection details

**Analysis File Download**:
The `download` action retrieves key files for session analysis:
- `current_state.json` - Current game state and memory
- `knowledgebase.md` - Accumulated strategic knowledge
- `zork_episode_log.jsonl` - Complete turn-by-turn episode log

Files are downloaded to timestamped `analysis_YYYYMMDD_HHMMSS/` directories for organized review.

## Project Documentation
- The [README.md](mdc:README.md) should be documented in manner that is just explaining the project, not providing examples of how to run it
- Use minimal adjectives, just be straight-forward
